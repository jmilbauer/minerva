{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide on some vocabulary. Articles are called articles. In wikipedia, links have two parts: a surface form, and a title that they link to. We will refer to the surface form of the link as the \"surface,\" and the link component as the \"title.\" These map loosely to the Saussurian idea of the signifier and the signified. The goal of this project is to, for every phrase that is ever a signifier on Wikipedia, identify that which it signifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from xml.etree import ElementTree as ET\n",
    "from bz2 import BZ2File\n",
    "import time\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import wikitextparser as wtp\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import multiprocessing as mltp\n",
    "from multiprocessing import Pool\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiroot = Path() / \"..\" / \"wiki_data\"\n",
    "wikipaths = list(wikiroot.glob(\"**/wikilines-100000*.txt\"))\n",
    "wikifiles = [p.resolve() for p in wikipaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embroot = Path() / \"..\" / \"downloads\"\n",
    "embpath = embroot / \"glove.6B.{}d.txt\".format(EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = {}\n",
    "for line in open(embpath.resolve(), 'r'):\n",
    "    parts = line.split()\n",
    "    embedding[parts[0]] = np.asarray(parts[1:]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_regex = \"\\{\\{([^\\|\\}]*)\\|?([^\\|\\}]*)\\}\\}\"\n",
    "wikilink_regex = \"\\[\\[([^\\|\\]]*)\\|?([^\\|\\]]*)\\]\\]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redirect_table = {} # Map String String, the correct redirect for each entry.\n",
    "# title_freq = {} # Map String Int\n",
    "# surface_freq = {} # Map String Int\n",
    "# surface_title_freq = {} # frequency of a specific surface form linking to a specific title. Map (String, String) Int\n",
    "# topics = {} # Map Title [Category]\n",
    "# avg_context = {} # Map Title (NP.Array, Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(root, prefix):\n",
    "    print(\"{}<{}>\".format(prefix, root.tag))\n",
    "    for child in root:\n",
    "        pretty_print(child, \"\\t{}\".format(prefix))\n",
    "    if root.text != None:\n",
    "        print(\"\\t{}{}\".format(prefix, root.text))\n",
    "    print(\"{}</{}>\".format(prefix, root.tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_article():\n",
    "#     return\n",
    "    \n",
    "# def process_redirect():\n",
    "#     return\n",
    "\n",
    "# def process_page(buffer):\n",
    "#     '''\n",
    "#     Processes a page.\n",
    "#     returns:\n",
    "#     0 if not an article, unexpected.\n",
    "#     1 if a disambiguation\n",
    "#     2 if a redirect\n",
    "#     3 if a content article.\n",
    "#     '''\n",
    "#     text = ' '.join(buffer)\n",
    "#     root = ET.fromstring(text)\n",
    "    \n",
    "#     ns = root.find('ns').text\n",
    "#     if ns != '0':\n",
    "#         return 0\n",
    "    \n",
    "#     name = root.find('title').text\n",
    "#     if name[-16:] == '(disambiguation)':\n",
    "#         return 1\n",
    "    \n",
    "#     redir = root.find('redirect')\n",
    "#     if redir != None:\n",
    "#         redir_to = redir.get('title')\n",
    "#         redirect_table[name] = redir_to\n",
    "#         return 2\n",
    "    \n",
    "#     recent_revision = root.find('revision')\n",
    "#     if recent_revision == None:\n",
    "#         return -1\n",
    "#     else:\n",
    "#         content = recent_revision.find('text')\n",
    "#         if content == None:\n",
    "#             return -1\n",
    "#         else:\n",
    "#             text = content.text\n",
    "# #             sentences = sent_tokenize(text)\n",
    "#             return 3\n",
    "            \n",
    "#     return -1\n",
    "\n",
    "# def process_wikifile(wikifile, verbose=False, verbosity=1000000):\n",
    "#     '''\n",
    "#     Takes a path to a wikipedia download file and processes it line by line.\n",
    "#     '''\n",
    "#     total_line_count = 0\n",
    "#     total_page_count = 0\n",
    "    \n",
    "#     article_count = 0\n",
    "#     redirect_count = 0\n",
    "#     disambiguation_count = 0\n",
    "#     other_count = 0\n",
    "#     error_count = 0\n",
    "    \n",
    "#     buffering = False\n",
    "#     buffer = []\n",
    "#     last = time.time()\n",
    "#     for line in BZ2File(wikifile, 'r'):\n",
    "#         line = line.decode('utf-8').strip()\n",
    "\n",
    "#         total_line_count += 1\n",
    "#         if verbose and total_line_count % verbosity == 0:\n",
    "#             now = time.time()\n",
    "#             print(\"Total Lines: {}\".format(total_line_count))\n",
    "#             print(\"Total articles: {}, Total redirects: {}, Total disambiguation: {}, Total other: {}, Total errors: {}\".format(article_count, redirect_count, disambiguation_count, other_count, error_count))\n",
    "#             print(\"Time for last {} lines: {:0.2f}s\".format(verbosity, now - last))\n",
    "#             print(\"Lines per second: {:0.2f}\".format(verbosity / (now-last)))\n",
    "#             last = time.time()\n",
    "\n",
    "#         if line == '<page>':\n",
    "#             total_page_count += 1\n",
    "#             buffering = True\n",
    "        \n",
    "#         if buffering:\n",
    "#             buffer.append(line)\n",
    "\n",
    "#         if line == \"</page>\":\n",
    "#             buffering = False\n",
    "#             code = process_page(buffer)\n",
    "#             if code == -1: error_count += 1\n",
    "#             elif code == 0: other_count += 1\n",
    "#             elif code == 1: disambiguation_count += 1\n",
    "#             elif code == 2: redirect_count += 1\n",
    "#             elif code == 3: article_count += 1\n",
    "#             buffer = []\n",
    "    \n",
    "#     print(\"Processed {}\".format(wikifile))\n",
    "#     print(\"Found {} pages in {} lines.\".format(total_page_count, total_line_count))\n",
    "    \n",
    "# def print_wikilinefile(buffer, apf, idx):\n",
    "#     nm = 'wikilines-{}-{}.txt'.format(apf, idx)\n",
    "#     outpath = wikiroot / \"wikilines\" / nm\n",
    "#     with open(outpath, 'w+') as fp:\n",
    "#         for line in buffer:\n",
    "#             fp.write(line)\n",
    "#             fp.write('\\n')\n",
    "#     return nm\n",
    "    \n",
    "# def wikifile_to_wikilinefiles(wikifile, articles_per_file):\n",
    "#     buffering = False\n",
    "#     buffer = []\n",
    "#     big_buffer = []\n",
    "#     print_index = 0\n",
    "#     buffered = 0\n",
    "#     apf = articles_per_file\n",
    "#     for line in BZ2File(wikifile, 'r'):\n",
    "#         line = line.decode('utf-8').strip()\n",
    "        \n",
    "#         if line == \"<page>\":\n",
    "#             buffering = True\n",
    "            \n",
    "#         if buffering:\n",
    "#             buffer.append(line)\n",
    "            \n",
    "#         if line == \"</page>\":\n",
    "#             buffering = False\n",
    "#             big_buffer.append(' '.join(buffer))\n",
    "#             buffered += 1\n",
    "#             buffer = []\n",
    "            \n",
    "#         if buffered == apf:\n",
    "#             n = print_wikilinefile(big_buffer, apf, print_index)\n",
    "#             print(\"Printed {} pages to {}\".format(apf, n))\n",
    "#             print_index += 1\n",
    "#             buffered = 0\n",
    "#             big_buffer = []\n",
    "            \n",
    "#     n = print_wikilinefile(big_buffer, apf, print_index)\n",
    "#     print(\"Printed {} pages to {}\".format(len(big_buffer), n))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiStats(object):\n",
    "    def __init__(self):\n",
    "        # Surface = String\n",
    "        # Title = String\n",
    "        # Category = String\n",
    "        self.redirect_table = {} # Map String -> Title\n",
    "        self.title_freq = {} # Map Title -> Int\n",
    "        self.surface_freq = {} # Map Surface -> Int\n",
    "        self.surface_title_freq = {} # Map Surface -> (Map Title -> Int)\n",
    "        self.context_sums = {} # Map Title -> NP.Vec\n",
    "        self.context_count = {} # Map Title -> Int\n",
    "        self.topics = {} # Map Title -> [Category]\n",
    "        self.titles = set([])\n",
    "        self.surfaces = set([])\n",
    "        self.__cleaned = False\n",
    "        \n",
    "    def get_average_context(self, title):\n",
    "        return self.context_sums[title] / self.context_count[title]\n",
    "    \n",
    "    def get_title_freq(self, title):\n",
    "        title = title.lower()\n",
    "        res = 0\n",
    "        if title in self.title_freq:\n",
    "            res += self.title_freq[title]\n",
    "        if title in self.redirect_table:\n",
    "            redir_to = self.redirect_table[title]\n",
    "            if redir_to in self.title_freq:\n",
    "                res += self.title_freq[redir_to]\n",
    "        return res\n",
    "    \n",
    "    def get_title_prob(self, title):\n",
    "        return self.title_freq[title] / sum(self.title_freq.values())\n",
    "    \n",
    "    def get_surface_title_prob(self, surface, title):\n",
    "        return self.surface_title_freq[surface][title] / sum(self.surface_title_freq[surface].values())\n",
    "    \n",
    "    def get_title_probs(self):\n",
    "        res = {}\n",
    "        titles = self.titles\n",
    "        total_mass = sum(self.title_freq.values())\n",
    "        for title in titles:\n",
    "            mass = self.title_freq[title]\n",
    "            target = self.redirect_table[title] if title in self.redirect_table else title\n",
    "            \n",
    "            if target not in res:\n",
    "                res[target] = 0\n",
    "            res[target] += mass / total_mass\n",
    "        return res\n",
    "    \n",
    "    def get_surface_probs(self):\n",
    "        res = {}\n",
    "        surfaces = self.surfaces\n",
    "        total_mass = sum(self.surface_freq.values())\n",
    "        for surface in surfaces:\n",
    "            mass = self.surface_freq[surface]            \n",
    "            res[surface] += mass / total_mass\n",
    "        return res\n",
    "    \n",
    "    def get_surface_title_probs(self):\n",
    "        res = {}\n",
    "        titles = self.titles\n",
    "        surfaces = self.surfaces\n",
    "        for surface in surfaces:\n",
    "            if surface not in res:\n",
    "                res[surface] = {}\n",
    "            \n",
    "            total_mass = sum(self.surface_title_freq[surface].values())\n",
    "            for title in self.surface_title_freq[surface]:\n",
    "                mass = self.surface_title_freq[surface][title]\n",
    "                target = self.redirect_table[title] if title in self.redirect_table else title\n",
    "                \n",
    "                if target not in res[surface]:\n",
    "                    res[surface][target] = 0\n",
    "                res[surface][target] += mass / total_mass\n",
    "        return res\n",
    "                \n",
    "    def get_avg_contexts(self):\n",
    "        res = {}\n",
    "        titles = self.titles\n",
    "        for title in titles:\n",
    "            avg_context = self.context_sums[title] / self.context_count[title]\n",
    "            target = self.redirect_table[title] if title in self.redirect_table else title\n",
    "            \n",
    "            if target not in res:\n",
    "                res[target] = avg_context\n",
    "            else:\n",
    "                res[target] += avg_context\n",
    "        return res    \n",
    "    \n",
    "    def merge(self, other):\n",
    "        self.redirect_table = {} # Map String -> Title\n",
    "        self.title_freq = {} # Map Title -> Int\n",
    "        self.surface_freq = {} # Map Surface -> Int\n",
    "        self.surface_title_freq = {} # Map Surface -> (Map Title -> Int)\n",
    "        self.context_sums = {} # Map Title -> NP.Vec\n",
    "        self.context_count = {} # Map Title -> Int\n",
    "        self.topics = {} # Map Title -> [Category]\n",
    "        self.titles = set([])\n",
    "        self.surfaces = set([])\n",
    "        \n",
    "        self.titles = self.titles.union(other.titles)\n",
    "        self.surfaces = self.surfaces.union(other.surfaces)\n",
    "        \n",
    "        for source in other.redirect_table:\n",
    "            if source in self.redirect_table:\n",
    "                print(\"Redirect collision: {} -> {} | {}\".format(source, other.redirect_table[source], self.redirect_table[source]))\n",
    "            self.redirect_table[source] = other.redirect_table[source]\n",
    "            \n",
    "        for title in other.title_freq:\n",
    "            if title in self.title_freq:\n",
    "                self.title_freq[title] += other.title_freq[title]\n",
    "            else:\n",
    "                self.title_freq[title] = other.title_freq[title]\n",
    "                \n",
    "        for surface in other.surface:\n",
    "            if surface in self.surface_freq:\n",
    "                self.surface_freq[surface] += other.surface_freq[surface]\n",
    "            else:\n",
    "                self.surface_freq[surface] = other.surface_freq[surface]\n",
    "                \n",
    "        for surface in other.surface_title_freq:\n",
    "            for title in other.surface_title_freq[surface]:\n",
    "                if surface in self.surface_title_freq:\n",
    "                    if title in self.surface_title_freq[surface]:\n",
    "                        self.surface_title_freq[surface][title] += other.surface_title_freq[surface][title]\n",
    "                    else:\n",
    "                        self.surface_title_freq[surface][title] = other.surface_title_freq[surface][title]\n",
    "                else:\n",
    "                    self.surface_title_freq[surface] = other.surface_title_freq[surface]\n",
    "                    \n",
    "        for title in other.context_sums:\n",
    "            if title in self.context_sums:\n",
    "                self.context_sums[title] += other.context_sums[title]\n",
    "            else:\n",
    "                self.context_sums[title] = other.context_sums[title]\n",
    "                \n",
    "        for title in other.context_count:\n",
    "            if title in self.context_count:\n",
    "                self.context_count[title] += other.context_count[title]\n",
    "            else:\n",
    "                self.context_count[title] = other.context_count[title]\n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spans(text, start, ends):\n",
    "        depth = 0\n",
    "        res = \"\"\n",
    "        ignoring = 0\n",
    "        for i in range(len(text)):\n",
    "            if text[i:i+len(start)] == start:\n",
    "                depth += 1 \n",
    "            if depth == 0:\n",
    "                if ignoring == 0:\n",
    "                    res += text[i]\n",
    "                else:\n",
    "                    ignoring -= 1\n",
    "            for end in ends:\n",
    "                if text[i:i+len(end)] == end:\n",
    "                    depth -= 1\n",
    "                    ignoring += len(end) - 1\n",
    "                    break\n",
    "        return res\n",
    "\n",
    "def to_surface(text, verbose=False):\n",
    "    '''\n",
    "    Takes a MediaWiki sentence and produces a simplified surface form.\n",
    "    '''\n",
    "    \n",
    "    def repl_fn(m):\n",
    "        if m.group(2) == '':\n",
    "            return m.group(1)\n",
    "        else:\n",
    "            return m.group(2)\n",
    "    \n",
    "    if verbose: print(text)\n",
    "    text = remove_spans(text, '{{', ['}};', '}}'])\n",
    "    text = remove_spans(text, '<ref', ['</ref>', '/>'])\n",
    "    text = remove_spans(text, '[[Image:', [']]'])\n",
    "    text = remove_spans(text, '[[File:', [']]'])\n",
    "    text = re.sub(wikilink_regex, repl_fn, text)\n",
    "    if verbose: print(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize(sentence):\n",
    "    '''\n",
    "    Tokenizes a wikipedia style sentence. Takes unprocessed wikipedia text and transforms it to a lowercase tokenized form\n",
    "    '''\n",
    "    return [w.lower() for w in word_tokenize(sentence)]\n",
    "\n",
    "def embed_sentence(tokens):\n",
    "    '''\n",
    "    Takes in a list of tokens, returns a sentence embedding\n",
    "    @tokens a list of word tokens, all lowercase, all nice words.\n",
    "    @return a numpy array of size EMBEDDING_DIMENSION containing a sentence embedding.\n",
    "    '''\n",
    "    token_vecs = np.array([embedding[tk] for tk in tokens if tk in embedding])\n",
    "    if len(token_vecs) == 0:\n",
    "        return np.zeros((EMBEDDING_DIMENSION,))\n",
    "    return np.mean(token_vecs, axis=0)\n",
    "\n",
    "def section_break(text):\n",
    "    '''\n",
    "    Takes an input of text and breaks it into a list of sections.\n",
    "    '''\n",
    "    sections = []\n",
    "    buffer = \"\"\n",
    "    minibuf = \"\"\n",
    "    buffering = True\n",
    "    for i in range(len(text)-1):\n",
    "        c = text[i]\n",
    "        if buffering and c == '=' and text[i+1] == '=':\n",
    "            buffering = False\n",
    "            \n",
    "        if buffering:\n",
    "            buffer += c\n",
    "        else:\n",
    "            minibuf += c\n",
    "            \n",
    "        if len(minibuf) >= 5:\n",
    "            if minibuf[:3] == '===':\n",
    "                if minibuf[-3:] == '===':\n",
    "                    sections.append(buffer)\n",
    "                    buffer = ''\n",
    "                    minibuf = ''\n",
    "                    buffering = True\n",
    "            elif minibuf[:2] == '==' and minibuf[-2:] == '==':\n",
    "                sections.append(buffer)\n",
    "                buffer = ''\n",
    "                minibuf = ''\n",
    "                buffering = True\n",
    "                \n",
    "    sections.append(buffer + minibuf)\n",
    "    return sections\n",
    "    \n",
    "def parse_wiki(text):\n",
    "    '''\n",
    "    Takes in a string of wiki markup from a single article.\n",
    "    Returns list of link info tuples: (surface, title, sentence embedding).\n",
    "    @text An unprocessed wiki markup string, from one article.\n",
    "    @return (a,b,c)\n",
    "    @return a List of sentences as a string. Unprocessed.\n",
    "    @return a list of links, minus the categories, to lowercase\n",
    "    @return c List of categories, to lowercase.\n",
    "    '''\n",
    "    sections = section_break(text)\n",
    "    res = []\n",
    "    for section in sections:\n",
    "        sentences = sent_tokenize(section.strip())\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            parsed = wtp.parse(sentence)\n",
    "            links = parsed.wikilinks\n",
    "            surface = to_surface(sentence)\n",
    "            if len(surface) > 0:\n",
    "                tokens = tokenize(surface)\n",
    "                embedding = embed_sentence(tokens)\n",
    "                for link in links:\n",
    "                    if link.text != None and ('|' in link.text or '[[' in link.text):\n",
    "                        pass\n",
    "                    else:\n",
    "                        res.append((link.text, link.title, embedding))\n",
    "    return res \n",
    "    \n",
    "def process_text(name, text, wikistats):\n",
    "    link_data = parse_wiki(text)\n",
    "    for (surface, title, embedding) in link_data:\n",
    "        \n",
    "        wikistats.titles.add(title)\n",
    "        wikistats.surfaces.add(surface)\n",
    "        \n",
    "        if title not in wikistats.title_freq:\n",
    "            wikistats.title_freq[title] = 0\n",
    "        wikistats.title_freq[title] += 1\n",
    "        \n",
    "        if surface not in wikistats.surface_freq:\n",
    "            wikistats.surface_freq[surface] = 0\n",
    "        wikistats.surface_freq[surface] += 1\n",
    "        \n",
    "        if surface not in wikistats.surface_title_freq:\n",
    "            wikistats.surface_title_freq[surface] = {}\n",
    "        if title not in wikistats.surface_title_freq[surface]:\n",
    "            wikistats.surface_title_freq[surface][title] = 0\n",
    "        wikistats.surface_title_freq[surface][title] += 1\n",
    "        \n",
    "        if title not in wikistats.context_sums:\n",
    "            wikistats.context_sums[title] = np.zeros((EMBEDDING_DIMENSION,))\n",
    "            wikistats.context_count[title] = 0\n",
    "        wikistats.context_sums[title] += embedding\n",
    "        wikistats.context_count[title] += 1\n",
    "    return wikistats\n",
    "            \n",
    "def process_xml(root, wikistats):\n",
    "    '''\n",
    "    Given a tree for an article, process it and update our tables of information.\n",
    "    '''\n",
    "    name = root.find('title').text.lower()\n",
    "    \n",
    "    if 'isambiguation' in name:\n",
    "        return ('disambiguation', wikistats)\n",
    "    \n",
    "    redir = root.find('redirect')\n",
    "    if redir != None:\n",
    "        redir_to = redir.get('title').lower()\n",
    "        if name != redir_to:\n",
    "            wikistats.redirect_table[name] = redir_to\n",
    "        return ('redirect', wikistats)\n",
    "    \n",
    "    recent_rev = root.find('revision')\n",
    "    if recent_rev != None:\n",
    "        content = recent_rev.find('text')\n",
    "        if content != None:\n",
    "            text = content.text.lower()\n",
    "            wikistats = process_text(name, text, wikistats)\n",
    "            return ('article', wikistats)\n",
    "        \n",
    "    return ('err', wikistats)\n",
    "\n",
    "def process_wikilinefile(wikilinefile, verbose=False, cutoff=None):\n",
    "#     redirect_table = {} # Map String String, the correct redirect for each entry.\n",
    "#     title_freq = {} # Map String Int\n",
    "#     surface_freq = {} # Map String Int\n",
    "#     surface_title_freq = {} # frequency of a specific surface form linking to a specific title. Map (String, String) Int\n",
    "#     avg_context = {} # Map Title (NP.Array, Int) || array is a sum, int is the total count contributing to the sum.\n",
    "#     topics = {} # Map Title [Category]\n",
    "    wikistats = WikiStats()\n",
    "    \n",
    "    total_pages = 0\n",
    "    total_articles = 0\n",
    "    total_redirects = 0\n",
    "    total_disambiguation = 0\n",
    "    total_others = 0\n",
    "    for line in open(wikilinefile, 'r'):\n",
    "        root = ET.fromstring(line)\n",
    "        total_pages += 1\n",
    "        if total_pages % 10000 == 0:\n",
    "            print(\"{}: Pages: {}\".format(wikilinefile, total_pages))\n",
    "        if cutoff != None and total_pages >= cutoff:\n",
    "            return {'total_page_count' : total_pages,\n",
    "                    'article_count' : total_articles,\n",
    "                    'redirect_count' : total_redirects,\n",
    "                    'disambiguation_count' : total_disambiguation,\n",
    "                    'other_count' : total_others,\n",
    "                    'wikistats' : wikistats}\n",
    "        \n",
    "        ns = root.find('ns').text\n",
    "        if ns != '0':\n",
    "            continue\n",
    "        \n",
    "        page_type, wikistats = process_xml(root, wikistats)\n",
    "        if page_type == 'article': total_articles += 1\n",
    "        elif page_type == 'redirect': total_redirects += 1\n",
    "        elif page_type == 'disambiguation': total_disambiguation += 1\n",
    "        else: total_others += 1\n",
    "            \n",
    "    return {'total_page_count' : total_pages,\n",
    "            'article_count' : total_articles,\n",
    "            'redirect_count' : total_redirects,\n",
    "            'disambiguation_count' : total_disambiguation,\n",
    "            'other_count' : total_others,\n",
    "            'wikistats' : wikistats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(filepath):\n",
    "    start = time.time()\n",
    "    print(\"Getting stats from: {}\".format(filepath))\n",
    "    stats = process_wikilinefile(filepath)\n",
    "    print(\"Got stats from: {} in {}s\".format(filepath, time.time() -start))\n",
    "    return stats\n",
    "\n",
    "mltp.cpu_count()\n",
    "pool = Pool(processes=13)\n",
    "\n",
    "stat_objects = pool.map(get_stats, wikifiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process_wikifile(wikifile, verbose=True, verbosity=100000)\n",
    "# # wikifile_to_wikilinefiles(wikifile, 100000)\n",
    "# start = time.time()\n",
    "# stats = process_wikilinefile(wikifiles[0])\n",
    "# print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikistats = stats['wikistats']\n",
    "title_probs = wikistats.get_title_probs()\n",
    "surface_title_probs = wikistats.get_surface_title_probs()\n",
    "avg_contexts = wikistats.get_avg_contexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = lambda x,y: np.linalg.norm(x-y)\n",
    "cosine = lambda x,y: sp.spatial.distance.cosine(x,y)\n",
    "\n",
    "def distance_distribution(my_emb, avg_contexts, candidates, metric=euclidean, pwr=1):\n",
    "    '''\n",
    "    For each surface -> title, produce a distribution over titles based on context distance rather than raw count.\n",
    "    '''\n",
    "    distances = {}\n",
    "    for candidate in candidates:\n",
    "        distances[candidate] = metric(my_emb, avg_contexts[candidate]) ** pwr\n",
    "    total_mass = sum(distances.values())\n",
    "    for candidate in candidates:\n",
    "        distances[candidate] /= total_mass\n",
    "    return distances\n",
    "\n",
    "def score(tokens, surface, title_probs, surface_title_probs, avg_contexts):\n",
    "    candidates = surface_title_probs[surface]\n",
    "    my_context = embed_sentence(tokens)\n",
    "    dists = distance_distribution(my_context, avg_contexts, candidates, pwr=1)\n",
    "    scored_candidates = []\n",
    "    for candidate in candidates:\n",
    "        p_title = title_probs[candidate]\n",
    "        p_surface_title = surface_title_probs[surface][candidate]\n",
    "        p_context = dists[candidate]\n",
    "        scored_candidates.append((p_context, p_title * p_surface_title * p_context, candidate))\n",
    "    return sorted(scored_candidates)\n",
    "\n",
    "def tokenize(sent):\n",
    "    return word_tokenize(sent.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"2004 was an incredible year for basketball, with michael jordan scoring highly across the board\"\n",
    "tokens = tokenize(sentence)\n",
    "scores = score(tokens, 'basketball', title_probs, surface_title_probs, avg_contexts)\n",
    "\n",
    "for a in scores:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_titles = {}\n",
    "\n",
    "# for title in wikistats.title_freq:\n",
    "#     if wikistats.title_freq[title] > 500:\n",
    "#         avg_context = wikistats.get_average_context(title)\n",
    "#         common_titles[title] = avg_context\n",
    "            \n",
    "# for title in common_titles:\n",
    "#     print(title)\n",
    "#     vec = common_titles[title]\n",
    "#     dists = [(np.linalg.norm(vec - v), k) for (k,v) in common_titles.items()]\n",
    "#     print(sorted(dists)[:5])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for surface in wikistats.surface_title_freq:\n",
    "#     if surface == None:\n",
    "#         continue\n",
    "#     if len(wikistats.surface_title_freq[surface]) > 3:\n",
    "#         print(\"{}: {}\".format(surface, wikistats.surface_title_freq[surface]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def choose_title(tokens, surface, wikistats):\n",
    "#     candidates = wikistats.surface_title_freq[surface]\n",
    "#     my_context = embed_sentence(tokens)\n",
    "#     scored_candidates = []\n",
    "#     for candidate in candidates:\n",
    "#         avg_context = wikistats.get_average_context(candidate)\n",
    "#         p_title = wikistats.get_title_prob(candidate)\n",
    "#         p_surface_title = wikistats.get_surface_title_prob(surface, candidate)\n",
    "#         dist = np.linalg.norm(avg_context - my_context)\n",
    "#         scored_candidates.append((dist, p_surface_title * p_title, candidate))\n",
    "#     print(scored_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Since the dawn of basketball, Chicago has been a pioneering team\"\n",
    "# def tokenize(sentence):\n",
    "#     return word_tokenize(sentence.lower())\n",
    "# tokens = tokenize(sentence)\n",
    "# print(tokens)\n",
    "# choose_title(tokens, \"chicago\", wikistats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
