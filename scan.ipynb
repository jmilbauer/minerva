{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import neccessary packages and point to the datasets on the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree as ET\n",
    "from html import unescape\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import time\n",
    "from playsound import playsound\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "dataroot = Path.home() / \"Documents\" / \"Data\"\n",
    "glovepath = dataroot / \"glove.6B\" / \"glove.6B.50d.txt\"\n",
    "wikipath = dataroot / \"wikipedia\" / \"enwiki-20191220.xml.bz2\"\n",
    "\n",
    "glovefile = glovepath.resolve()\n",
    "wikifile = wikipath.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the information we're extracting from wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redirect_table = {} # table of all redirects. used for merging the tables below.\n",
    "title_freq = {} # table of the frequency of links to each article.\n",
    "anchor_title_freq = {} # table of Freq(anchor | title). So A[i][j] = COUNT(j | i)\n",
    "title_contexts = {} # the KMEANS++\n",
    "category_table = {} # Map Title Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we iterate over wikipedia to fill out those tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed = {}\n",
    "count = 0\n",
    "with open(glovefile, 'r') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        embed[parts[0]] = np.array(list(map(float, parts[1:])))\n",
    "        \n",
    "def embed_word(w):\n",
    "    w = w.lower()\n",
    "    if w in embed:\n",
    "        return embed[w]\n",
    "    else:\n",
    "        return embed['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.34456667e-01  5.11975000e-03  3.81589167e-02 -2.62422250e-01\n",
      "  5.15089167e-01 -1.61305833e-02 -8.18162500e-01 -1.02467500e-01\n",
      " -3.13640872e-01 -7.94299525e-02 -5.48941667e-02  5.28040833e-02\n",
      " -2.48377667e-01 -2.57688750e-01  5.27354250e-01  1.62426083e-01\n",
      " -2.02633917e-01  6.17953333e-02 -3.48771167e-01 -2.53367583e-01\n",
      "  3.87532075e-01  2.63060333e-01  1.18577142e-01  1.22785667e-01\n",
      "  8.02510000e-02 -1.51455667e+00 -4.52058333e-01  4.73327500e-02\n",
      "  3.06048833e-01 -4.71920167e-01  2.92805042e+00  1.28444167e-01\n",
      " -1.47871000e-01  2.46600833e-02  2.36680355e-01  8.75444000e-02\n",
      " -7.18991667e-02  1.68826917e-01  2.70514833e-01  1.55518333e-02\n",
      " -2.72602500e-03  1.85462167e-01  3.49908333e-03  9.22332500e-02\n",
      " -3.85677500e-02  5.12871000e-02 -1.91620600e-01 -2.69056417e-01\n",
      " -1.96925833e-02 -2.62114917e-01]\n"
     ]
    }
   ],
   "source": [
    "def embed_wordlist(words, summary_fn):\n",
    "    return summary_fn([embed_word(w) for w in words])\n",
    "\n",
    "def embed_sentence(wordlist):\n",
    "    summary_fn = lambda x : np.mean(x, axis=0)\n",
    "    return embed_wordlist(wordlist, summary_fn)\n",
    "\n",
    "def cosine_sim(u,v):\n",
    "    return np.dot(u,v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines at 91684.92836568487 lines per second\n",
      "Processed 2000000 lines at 94758.33805876356 lines per second\n",
      "Found 10000 pages at 409.78580024335446 pages per second.\n",
      "Processed 3000000 lines at 96545.65990616998 lines per second\n",
      "Processed 4000000 lines at 96303.22249440274 lines per second\n",
      "Found 20000 pages at 408.94790979413125 pages per second.\n",
      "Processed 5000000 lines at 96244.45822170719 lines per second\n",
      "Processed 6000000 lines at 97702.52645452421 lines per second\n",
      "Found 30000 pages at 447.9189069665639 pages per second.\n",
      "Processed 7000000 lines at 98111.22971995671 lines per second\n",
      "Processed 8000000 lines at 98445.40297749237 lines per second\n",
      "Found 40000 pages at 474.44301666877817 pages per second.\n",
      "Processed 9000000 lines at 99196.20867436587 lines per second\n",
      "Found 50000 pages at 503.81198150210486 pages per second.\n",
      "Processed 10000000 lines at 100245.64689196595 lines per second\n",
      "Processed 11000000 lines at 101471.6432418664 lines per second\n",
      "Found 60000 pages at 539.4642845532758 pages per second.\n",
      "Processed 12000000 lines at 103208.38737934356 lines per second\n",
      "Processed 13000000 lines at 103720.5469486801 lines per second\n",
      "Found 70000 pages at 541.1152972024938 pages per second.\n",
      "Processed 14000000 lines at 104145.30580586496 lines per second\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-bade31de621a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'<page>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lines = 0\n",
    "pages = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "keeping = False\n",
    "articles = []\n",
    "buffer = \"\"\n",
    "for line in bz2.BZ2File(wikifile, 'r'):\n",
    "    line = line.decode('utf-8').strip()\n",
    "    lines += 1\n",
    "    if line == '<page>':\n",
    "        keeping = True\n",
    "    if keeping:\n",
    "        buffer += line\n",
    "    if keeping and line == '</page>':\n",
    "        keeping = False\n",
    "        pages += 1\n",
    "        process_article(buffer)\n",
    "        buffer = \"\"\n",
    "        if pages % 10000 == 0:\n",
    "            print(\"Found {} pages at {} pages per second.\".format(pages, pages / (time.time() - start)))\n",
    "    if lines % 1000000 == 0:\n",
    "        print(\"Processed {} lines at {} lines per second\".format(lines, lines / (time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We care about the following states:\n",
    "0: scanning, the default state.\n",
    "1: article-page\n",
    "\n",
    "If we are in an article-page state, we concatenate all lines...\n",
    "until we hit the </page> tag.\n",
    "\n",
    "Once the lines have been concatenated, we process the article-page.\n",
    "\n",
    "The article-page is either a DISAMBIGUATION, REDIRECT, or ARTICLE\n",
    "DISAMBIGUATION pages are largely ignored.\n",
    "REDIRECT pages update a redirect table\n",
    "ARTICLE pages have their individual sentences processed.\n",
    "\n",
    "Each sentence updates three things:\n",
    "title_freqs, representing the frequency of each article title appearing in an internal link.\n",
    "anchor_title_freqs, representing frequency of each anchor linking to a title.\n",
    "title_contexts, the set of all embedding contexts in which an article is linked.\n",
    "\"\"\"\n",
    "\n",
    "def get_title(title, block):\n",
    "    if '<title>' in block:\n",
    "        return block.strip()[7:-8]\n",
    "    else:\n",
    "        return title\n",
    "\n",
    "def get_state(state, line):\n",
    "    \"\"\"\n",
    "    This method determines the current state by examining a line.\n",
    "    If the line says namespace = 0, then we've entered a page of the article type.\n",
    "    If the line says /page, then the current page is over -- so if we're in the article type, we now exit.\n",
    "    \"\"\"\n",
    "    new_state = state\n",
    "    if state == 0 and '<ns>0</ns>' in line:\n",
    "        new_state = 1\n",
    "    if state == 1 and '</page>' in line:\n",
    "        new_state = 0\n",
    "    return new_state, state\n",
    "\n",
    "def string_at(txt, idx, substr):\n",
    "    start = idx\n",
    "    end = idx + len(substr)\n",
    "    if end >= len(txt):\n",
    "        return False\n",
    "    elif txt[start:end] == substr:\n",
    "        return True\n",
    "    else:\n",
    "        return False  \n",
    "    \n",
    "def remove_tags(text, tag_pairs):\n",
    "    res = []\n",
    "    looking_for = []\n",
    "    for i, c in enumerate(text):\n",
    "        for (a,b) in tag_pairs:\n",
    "            if string_at(text, i, a):\n",
    "                looking_for.append(b)\n",
    "        if looking_for == []:\n",
    "            res.append(c)\n",
    "        elif string_at(text, i-len(looking_for[-1])+1, looking_for[-1]):\n",
    "            looking_for = looking_for[:-1]\n",
    "    return ''.join(res)\n",
    "\n",
    "def clean_wiki(body_text):\n",
    "    body_text = unescape(body_text)\n",
    "    tags = [('{{','}}'),('<!--', '-->'),('<ref', '>'),('</ref','>'),('[[File:', ']]')]\n",
    "    body_text = remove_tags(body_text, tags)\n",
    "    body_text = body_text.strip()\n",
    "\n",
    "    return body_text\n",
    "\n",
    "def find_and_remove_categories(body_text):\n",
    "    skip_ahead = 0\n",
    "    reading = False\n",
    "    ignoring = False\n",
    "    categories = []\n",
    "    buffer = \"\"\n",
    "    body = \"\"\n",
    "    for i in range(len(body_text)):\n",
    "        if skip_ahead > 0:\n",
    "            skip_ahead -= 1\n",
    "            continue\n",
    "            \n",
    "        c = body_text[i]\n",
    "        if c == '[' and body_text[i+1] == '[' and string_at(body_text, i+2, 'Category:'):\n",
    "            skip_ahead = len('[Category:')\n",
    "            reading = True\n",
    "        elif reading == True:\n",
    "            if c == '|':\n",
    "                ignoring = True\n",
    "            if c == ']' and body_text[i+1] == ']':\n",
    "                categories.append(buffer.strip())\n",
    "                buffer = \"\"\n",
    "                reading = False\n",
    "                ignoring = False\n",
    "                skip_ahead += 1\n",
    "            elif not ignoring:\n",
    "                buffer += c\n",
    "        else:\n",
    "            body += c\n",
    "    return categories, body.strip()\n",
    "    \n",
    "def process_article(buffer, title):\n",
    "    full_xml = '\\n'.join(buffer)\n",
    "    root = ET.fromstring(\"<root>\" + full_xml + \"</root>\")\n",
    "    bodies = root.iter('text')\n",
    "#     xml = ET.tostring(root)\n",
    "    for body in bodies:\n",
    "        body_text = body.text\n",
    "        body_text = clean_wiki(body_text)\n",
    "        categories, body_text = find_and_remove_categories(body_text)\n",
    "        category_table[title] = categories\n",
    "        sentences = sent_tokenize(body_text)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            state = 0 #1=1open, 2=inlink, 3=insurface, 4=1close,\n",
    "            surface = \"\"\n",
    "            surface_sentence = \"\"\n",
    "            link = \"\"\n",
    "            links = []\n",
    "            for i,c in enumerate(sent):                    \n",
    "                if state == 0 and c == '[':\n",
    "                    state = 1\n",
    "                elif state == 1 and c == '[':\n",
    "                    state = 2\n",
    "                elif state == 2 and c != '|' and c != ']':\n",
    "                    link += c\n",
    "                elif state == 2 and c == '|':\n",
    "                    state = 3\n",
    "                elif state == 2 and c == ']':\n",
    "                    state = 4\n",
    "                    surface = link\n",
    "                elif state == 3 and c != ']':\n",
    "                    surface += c\n",
    "                elif state == 3 and c == ']':\n",
    "                    state = 4\n",
    "                elif state == 4 and c == ']':\n",
    "                    state = 0\n",
    "\n",
    "                    if link not in title_freq:\n",
    "                        title_freq[link] = 0\n",
    "                    title_freq[link] += 1\n",
    "\n",
    "                    if link not in anchor_title_freq:\n",
    "                        anchor_title_freq[link] = {}\n",
    "                    if surface not in anchor_title_freq[link]:\n",
    "                        anchor_title_freq[link][surface] = 0\n",
    "                    anchor_title_freq[link][surface] += 1\n",
    "\n",
    "                    surface_sentence += surface\n",
    "                    links.append(link)\n",
    "                    surface = \"\"\n",
    "                    link = \"\"\n",
    "                else:\n",
    "                    state = state\n",
    "                    surface_sentence += c\n",
    "                    \n",
    "            emb = embed_wordlist(surface_sentence.strip().lower().strip())\n",
    "            for link in links:\n",
    "                if link not in title_contexts:\n",
    "                    title_contexts[link] = []\n",
    "                title_contexts[link].append(emb)\n",
    "            \n",
    "            \n",
    "def process_redirect(buffer, title):\n",
    "    source = title\n",
    "    for x in buffer:\n",
    "        if '<redirect' in x:\n",
    "            target = x.strip()[17:-4]\n",
    "            if source not in redirect_table:\n",
    "                redirect_table[source] = target\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def process_disambiguation(buffer, title):\n",
    "    for x in buffer:\n",
    "        if '(disambiguation)' in x:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def process_contents(buffer, title, verbose=False):\n",
    "    \"\"\"\n",
    "    This method takes a list of article lines and processes them.\n",
    "    \"\"\"    \n",
    "    article_count = 0\n",
    "    if process_disambiguation(buffer, title):\n",
    "        if verbose:\n",
    "            print(\"Disambiguation: {}\".format(title))\n",
    "    \n",
    "    elif process_redirect(buffer, title):\n",
    "        if verbose:\n",
    "            print(\"Redirect: {} -> {}\".format(title, redirect_table[title]))\n",
    "        \n",
    "    else:\n",
    "        process_article(buffer, title)\n",
    "        article_count += 1\n",
    "        if verbose:\n",
    "            print(\"Article: {}\".format(title))\n",
    "    \n",
    "    return article_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_wikifile(wp, max_count):\n",
    "    start = time.time()\n",
    "    with bz2.BZ2File(wp, 'rb') as wiki_file:\n",
    "        state = 0\n",
    "        buffer = []\n",
    "        counter = 0\n",
    "        title = None\n",
    "        article_count = 0\n",
    "        while True:\n",
    "            # get the line.\n",
    "            block = wiki_file.readline().decode('utf-8')\n",
    "            if block == None or counter >= max_count:\n",
    "                break\n",
    "            counter += 1\n",
    "\n",
    "            # get the reader state, store contents, and process each block.\n",
    "            title = get_title(title, block)\n",
    "            state, old_state = get_state(state, block)\n",
    "            if state == 1:\n",
    "                # this state indicates we're in an article, and should keep track of content.\n",
    "                buffer.append(block.strip())\n",
    "            if old_state == 1 and state == 0:\n",
    "                # this case indicates a transition away from the article.\n",
    "                ac = process_contents(buffer, title, verbose=False)\n",
    "                article_count += ac\n",
    "                buffer = []\n",
    "    end = time.time()\n",
    "    return end - start, counter, article_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.955867767333984 100000 194\n"
     ]
    }
   ],
   "source": [
    "redirect_table = {} # table of all redirects. used for merging the tables below.\n",
    "title_freq = {} # table of the frequency of links to each article.\n",
    "anchor_title_freq = {} # table of Freq(anchor | title). So A[i][j] = COUNT(j | i)\n",
    "title_contexts = {} # the KMEANS++ if we need to compress. MAX neighbor otherwise.\n",
    "category_table = {}\n",
    "\n",
    "t, l, ac = process_wikifile(wiki_path, 100000)\n",
    "print(t, l, ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "33610\n",
      "33610\n",
      "194\n",
      "33610\n"
     ]
    }
   ],
   "source": [
    "print(len(redirect_table))\n",
    "print(len(anchor_title_freq))\n",
    "print(len(title_freq))\n",
    "print(len(category_table))\n",
    "print(len(title_contexts))\n",
    "\n",
    "for title in redirect_table:\n",
    "    if title in title_freq:\n",
    "        title_freq[redirect_table[title]] += title_freq[title]\n",
    "        title_freq[title] = 0\n",
    "    if title in anchor_title_freq:\n",
    "        for anchor in anchor_title_freq[title]:\n",
    "            anchor_title_freq[redirect_table[title]][anchor] += anchor_title_freq[title][anchor]\n",
    "            anchor_title_freq[title][anchor] = 0\n",
    "            \n",
    "title_anchor_freq = {}\n",
    "for title in anchor_title_freq:\n",
    "    for anchor in anchor_title_freq[title]:\n",
    "        if anchor not in title_anchor_freq:\n",
    "            title_anchor_freq[anchor] = {}\n",
    "        title_anchor_freq[anchor][title] = anchor_title_freq[title][anchor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_candidate_titles(link):\n",
    "    xs = title_anchor_freq[link].keys()\n",
    "    freqs = np.array([title_anchor_freq[link][x] for x in xs])\n",
    "    probs = freqs / sum(freqs)\n",
    "    return list(xs), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\n",
    "word = \"\"\n",
    "candidates, probs = get_candidate_titles(word)\n",
    "prob_table = {}\n",
    "\n",
    "def get_link_given_candidate(candidate, link):\n",
    "    link_freqs = [anchor_title_freq[candidate][x] for x in anchor_title_freq[candidate]]\n",
    "    return anchor_title_freq[candidate][link] / sum(link_freqs)\n",
    "\n",
    "def get_total_candidate_freq():\n",
    "    acc = 0\n",
    "    for x in title_freq:\n",
    "        acc += title_freq[x]\n",
    "    return acc\n",
    "\n",
    "def get_context_given_candidate(candidate, sentence):\n",
    "    wordlist = sentence.strip().lower().split()\n",
    "    emb = embed_wordlist(wordlist)\n",
    "    for context in title_contexts\n",
    "        \n",
    "for i, candidate in enumerate(candidates):\n",
    "    p_context_given_candidate =\n",
    "    p_link_given_candidate = get_link_given_candidate(candidate, word)\n",
    "    p_candidate = title_freq[candidate] / get_total_candidate_freq()\n",
    "#     p_link_given_context = ignored for now.\n",
    "    \n",
    "    \n",
    "    prob_table[candidate] = probs[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3]",
   "language": "python",
   "name": "Python [py3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
